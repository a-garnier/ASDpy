{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Création des fichiers DF.ipynb",
   "provenance": [],
   "authorship_tag": "ABX9TyPNRWAPm3PgWv16rDcpZlb+",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/a-garnier/ASDpy/blob/final/Cr%C3%A9ation_des_fichiers_DF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZ-R1s77wuA6"
   },
   "source": [
    "Ce script permet la création et la sauvegarde de l'ensemble des fichiers permettant de stocker les données sous forme de dataframe. Il permet donc de créer en tout 15 fichiers:\n",
    "df_fan_test.csv\n",
    "df_fan_train.csv\n",
    "df_valve_test.csvt\n",
    "df_valve_train.csv\n",
    "df_pump_test.csv\n",
    "df_pump_train.csv\n",
    "df_slider_test.csv\n",
    "df_slider_train.csv\n",
    "df_toycar_test.csv\n",
    "df_toycar_train.csv\n",
    "df_toyconveyor_test.csv\n",
    "df_toyconveyor_train.csv\n",
    "df_train.csv : toutes les machines des fichiers d'entrainement\n",
    "df_test.csv : toutes les machines des fichiers de tests\n",
    "df_total.csv : toutes les images Train+test\n",
    "\n",
    "L'objectif n'étant pas de maximiser l'utilisation des ressources disponibles mais bien de faciliter la répétition des tetsts sans recréer l'ensemble des dataframe from scratch.\n",
    "\n",
    "Pré-requis au lancement de ce script: l'ensemble des fichiers arrays doivent avoir été crés à partir de chaque image des spectogrammes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ENT0v0Q3yO_0"
   },
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "from joblib import dump, load\n",
    "\n",
    "\"\"\"Fonction permettant de créer un dataframe à partir d'un ensemble de fichiers dans un répertoire\"\"\"\n",
    "def CreateDataFrame(ArrayFolder):\n",
    "\n",
    "    #Scenario- Using a non Deep learning approach\n",
    "    #Create a dataframe for each equipment containing all the related files (the arrays), 1 array file = 1 line in the dataframe)\n",
    "\n",
    "    #if not os.path.exists(DataframeFolder):\n",
    "        #os.makedirs(DataframeFolder)\n",
    "\n",
    "    Imfiles = [f for f in listdir(ArrayFolder)if isfile(join(ArrayFolder,f))]\n",
    "    # wavfiles.remove('.DS_Store')\n",
    "\n",
    "    list=[]\n",
    "\n",
    "    i=0\n",
    "    Target_Ano=0\n",
    "    Machine=\"\"\n",
    "    ID=0\n",
    "    file=\"\"\n",
    "    Arrays_f = np.empty([0, 40064])\n",
    "    features=[]\n",
    "\n",
    "\n",
    "    for f in Imfiles:\n",
    "\n",
    "        print(i)\n",
    "        if f[-4:] != '.txt':\n",
    "            # ignore non .pngfiles\n",
    "            continue\n",
    "\n",
    "        Arrays = np.loadtxt(join(ArrayFolder,f),dtype=int,delimiter=\",\")\n",
    "        Arrays = Arrays.reshape(1,-1)\n",
    "        features.append(Arrays.tolist())\n",
    "\n",
    "        #Ajout de l'array\n",
    "\n",
    "\n",
    "        if f[0]==\"a\":\n",
    "          Target_Ano=1\n",
    "        else:\n",
    "          Target_Ano=0\n",
    "\n",
    "        Machine=machine\n",
    "        ID=f[-15:-13]\n",
    "        file=f[-12:-4]\n",
    "\n",
    "        list.append([Machine,ID,file,Target_Ano])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    df_features=pd.DataFrame(features)\n",
    "    df_features = pd.DataFrame(df_features[0].values.tolist())\n",
    "    df_features_Conv=df_features.apply(pd.to_numeric, downcast='unsigned')\n",
    "    df=pd.concat([pd.DataFrame(list,columns=['Machine','ID','file','Target_ano']),df_features_Conv],axis=1)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"Structure de l'arborescence à prendre compte en entrée de la fonction\"\"\"\n",
    "\n",
    "machines = ['fan', 'pump', 'slider', 'ToyCar', 'ToyConveyor', 'valve']\n",
    "sets = ['train_png_arrays', 'test_png_arrays']\n",
    "df=[\"df_fan_test\",\"df_pump_test\",\"df_slider_test\",\"df_ToyCar_test\",\"df_ToyConveyor_test\",\"df_valve_test\"]\n",
    "i=0\n",
    "\n",
    "\"\"\"Execution de la boucle permettant la création des fichiers\"\"\"\n",
    "\n",
    "for machine in machines:\n",
    "    for s in sets:\n",
    "      ArrayFolder='/content/drive/MyDrive/Projet Son/Data/'+machine+'/'+s+'/'\n",
    "      df[i]= CreateDataFrame(ArrayFolder)\n",
    "      i += 1\n",
    "\n",
    "#création et sauvegarde des fichiers CSV pour les 15 fichiers.\n",
    "\n",
    "df[0].to_csv('df_fan_train.csv', encoding='utf-8')\n",
    "!cp df_fan_train.csv \"drive/MyDrive/Projet Son/\"\n",
    "\n",
    "df[1].to_csv('df_fan_test.csv', encoding='utf-8')\n",
    "!cp df_fan_test.csv \"drive/MyDrive/Projet Son/\"\n",
    "\n",
    "df[2].to_csv('df_pump_train.csv', encoding='utf-8')\n",
    "!cp df_pump_train.csv \"drive/MyDrive/Projet Son/\"\n",
    "\n",
    "df[3].to_csv('df_pump_test.csv', encoding='utf-8')\n",
    "!cp df_pump_test.csv \"drive/MyDrive/Projet Son/\"\n",
    "\n",
    "df[4].to_csv('df_slider_train.csv', encoding='utf-8')\n",
    "!cp df_slider_train.csv \"drive/MyDrive/Projet Son/\"\n",
    "\n",
    "df[5].to_csv('df_slider_test.csv', encoding='utf-8')\n",
    "!cp df_slider_test.csv \"drive/MyDrive/Projet Son/\"\n",
    "\n",
    "df[6].to_csv('df_toycar_train.csv', encoding='utf-8')\n",
    "!cp df_toycar_train.csv \"drive/MyDrive/Projet Son/\"\n",
    "\n",
    "df[7].to_csv('df_toycar_test.csv', encoding='utf-8')\n",
    "!cp df_toycar_test.csv \"drive/MyDrive/Projet Son/\"\n",
    "\n",
    "df[8].to_csv('df_toyconveyor_train.csv', encoding='utf-8')\n",
    "!cp df_toyconveyor_train.csv \"drive/MyDrive/Projet Son/\"\n",
    "\n",
    "df[9].to_csv('df_toyconveyor_test.csv', encoding='utf-8')\n",
    "!cp df_toyconveyor_test.csv \"drive/MyDrive/Projet Son/\"\n",
    "\n",
    "df[10].to_csv('df_valve_train.csv', encoding='utf-8')\n",
    "!cp df_valve_train.csv \"drive/MyDrive/Projet Son/\"    \n",
    "\n",
    "df[11].to_csv('df_valve_test.csv', encoding='utf-8')\n",
    "!cp df_valve_test.csv \"drive/MyDrive/Projet Son/\"  \n",
    "\n",
    "df_train=pd.concat((df[0],df[2],df[4],df[6],df[8],df[10]),axis=0)\n",
    "df_train.to_csv('df_train.csv', encoding='utf-8')\n",
    "!cp df_train.csv \"drive/MyDrive/Projet Son/\" \n",
    "\n",
    "df_test=pd.concat((df[1],df[3],df[5],df[7],df[9],df[11]),axis=0)\n",
    "df_test.to_csv('df_test.csv', encoding='utf-8')\n",
    "!cp df_test.csv \"drive/MyDrive/Projet Son/\" \n",
    "\n",
    "df_total=pd.concat((df_train,df_test),axis=0)\n",
    "df_total.to_csv('df_total.csv', encoding='utf-8')\n",
    "!cp df_total.csv \"drive/MyDrive/Projet Son/\" "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "id": "BqZxR2dswqBz",
    "outputId": "6bd4261f-bfb8-463c-b4a3-1378f7d5238b"
   },
   "source": [
    ""
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<ipython-input-1-8da994048dbc>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    Ce script permet la création et la sauvegarde de l'ensemble des fichiers permettant de stocker les données sous forme de dataframe. Il permet donc de créer en tout 15 fichiers:\u001B[0m\n\u001B[0m            ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rBqIM0rByjkH"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ej0aJt1Oyhj6"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}